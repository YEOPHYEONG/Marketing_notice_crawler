import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import os
import time
from datetime import datetime
import json

# Google Sheets ì—°ë™ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# --- 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ---
PROCESSED_LINKS_FILE = 'processed_links.txt'

# --- 2. Google Sheets ì¸ì¦ ë° ë°ì´í„° ë¡œë“œ ---
def load_targets_from_sheets():
    """Google Sheetsì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    print("--- Google Sheetsì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ ë¡œë“œ ì‹œì‘ ---")
    try:
        # GitHub Secretì—ì„œ JSON ì¸ì¦ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        creds_json_str = os.environ.get('GOOGLE_API_CREDENTIALS')
        if not creds_json_str:
            print("âŒ GOOGLE_API_CREDENTIALS Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
        creds_dict = json.loads(creds_json_str)
        
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        
        # [â˜…â˜… ì¤‘ìš” â˜…â˜…] ì—¬ê¸°ì— ë³¸ì¸ì˜ Google Sheet íŒŒì¼ ì´ë¦„ì„ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.
        sheet_name = "ë‚˜ì˜ í¬ë¡¤ëŸ¬ ì„¤ì • ì‹œíŠ¸" 
        sheet = client.open(sheet_name).sheet1
        
        records = sheet.get_all_records()
        print(f"âœ… Google Sheetsì—ì„œ {len(records)}ê°œì˜ í¬ë¡¤ë§ ëŒ€ìƒì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        return records

    except Exception as e:
        print(f"âŒ Google Sheets ì—°ë™ ì‹¤íŒ¨: {e}")
        print("   (API ê¶Œí•œ, ì‹œíŠ¸ ê³µìœ , ì‹œíŠ¸ ì´ë¦„ ë“±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.)")
        return []

# --- 3. í¬ë¡¤ëŸ¬ í•µì‹¬ í•¨ìˆ˜ë“¤ ---

def send_email(subject, body, receiver_email):
    """ìš”ì•½ëœ ì´ë©”ì¼ì„ ë°œì†¡í•˜ëŠ” í•¨ìˆ˜."""
    print("\n--- ì´ë©”ì¼ ë°œì†¡ ì‹œë„ ---")
    try:
        # GitHub Secretsì—ì„œ ì´ë©”ì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        smtp_user = os.environ.get('GMAIL_USER')
        smtp_password = os.environ.get('GMAIL_PASSWORD')
        if not smtp_user or not smtp_password:
            print("âŒ GMAIL_USER ë˜ëŠ” GMAIL_PASSWORD Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
    except Exception as e:
        print(f"âŒ GitHub Secrets ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    msg = MIMEText(body, 'html', 'utf-8')
    msg['Subject'] = Header(subject, 'utf-8')
    msg['From'] = smtp_user
    msg['To'] = receiver_email

    try:
        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls()
            server.login(smtp_user, smtp_password)
            server.sendmail(msg['From'], [msg['To']], msg.as_string())
        print(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {subject}")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def load_processed_links():
    """ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬ ëª©ë¡ì„ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if not os.path.exists(PROCESSED_LINKS_FILE):
        return set()
    with open(PROCESSED_LINKS_FILE, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f)

def save_processed_link(link):
    """ìƒˆë¡­ê²Œ ì²˜ë¦¬ëœ ë§í¬ë¥¼ íŒŒì¼ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    with open(PROCESSED_LINKS_FILE, 'a', encoding='utf-8') as f:
        f.write(link + '\n')

def generate_summary_email_body(announcements):
    """ê³µê³  ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ HTML ì´ë©”ì¼ ë³¸ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    html = """
    <head>
        <style>
            body { font-family: 'Malgun Gothic', sans-serif; } .container { border: 1px solid #ddd; padding: 20px; margin: 20px; border-radius: 8px; } h2 { color: #005AAB; } table { width: 100%; border-collapse: collapse; } th, td { border: 1px solid #ddd; padding: 12px; text-align: left; } th { background-color: #f2f2f2; } a { color: #005AAB; text-decoration: none; } a:hover { text-decoration: underline; } .footer { margin-top: 20px; font-size: 12px; color: #888; }
        </style>
    </head>
    <body>
        <div class="container">
            <h2>ğŸ“¢ ì‹ ê·œ ì…ì°° ê³µê³  ìš”ì•½</h2>
            <p><strong>""" + datetime.now().strftime('%Yë…„ %mì›” %dì¼') + """</strong>ì— ë°œê²¬ëœ ì‹ ê·œ ê³µê³  ëª©ë¡ì…ë‹ˆë‹¤.</p>
            <table><thead><tr><th>íšŒì‚¬ëª…</th><th>ê³µê³  ì œëª©</th></tr></thead><tbody>
    """
    for ann in announcements:
        html += f"""<tr><td>{ann['company']}</td><td><a href="{ann['href']}">{ann['title']}</a></td></tr>"""
    html += """
            </tbody></table>
            <p class="footer">ë³¸ ë©”ì¼ì€ ìë™í™”ëœ ìŠ¤í¬ë¦½íŠ¸ì— ì˜í•´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        </div>
    </body>
    """
    return html

def crawl_site(target, keywords, processed_links):
    """ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ìƒˆë¡œìš´ ê³µê³  ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    company = target.get('company', 'N/A')
    url = target.get('url')
    selector = target.get('selector')
    base_url = target.get('base_url', '')
    new_announcements = []

    if not all([url, selector]):
        print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì˜ url ë˜ëŠ” selectorê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return new_announcements
        
    print(f"\n--- '{company}' ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ---")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âŒ '{company}' ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨: {e}")
        return new_announcements

    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.select(selector)

    if not links:
        print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì—ì„œ '{selector}' ì„ íƒìì— í•´ë‹¹í•˜ëŠ” ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return new_announcements

    for link in links:
        title = link.get_text(strip=True)
        href = link.get('href', '')

        if href and not href.startswith('http'):
            href = base_url.rstrip('/') + '/' + href.lstrip('/')

        if any(keyword.lower() in title.lower() for keyword in keywords) and href and href not in processed_links:
            print(f"ğŸš€ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬: [{company}] {title}")
            new_announcements.append({"company": company, "title": title, "href": href})
            save_processed_link(href)
            processed_links.add(href)
    
    if not new_announcements:
        print(f"â„¹ï¸ '{company}'ì—ì„œ í‚¤ì›Œë“œì— ë§ëŠ” ìƒˆë¡œìš´ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return new_announcements

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print("="*50)
    print("Google Sheets ì—°ë™ ì…ì°° ê³µê³  í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("="*50)
    
    targets = load_targets_from_sheets()
    if not targets:
        print("í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ì–´ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # [â˜…â˜… ì¤‘ìš” â˜…â˜…] ì•„ë˜ í‚¤ì›Œë“œì™€ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
    keywords_to_find = ["ëŒ€í–‰ì‚¬", "ì…ì°°", "ì„ ì •", "ê³µê³ ", "ëª¨ì§‘", "ë§ˆì¼€íŒ…"]
    email_to_receive = "gooodong3@gmail.com"
    
    processed_links = load_processed_links()
    all_new_announcements = []

    for target in targets:
        new_finds = crawl_site(target, keywords_to_find, processed_links)
        if new_finds:
            all_new_announcements.extend(new_finds)
        time.sleep(1) # ì‚¬ì´íŠ¸ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì§€ì—°

    print("\n--- ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ ---")

    if all_new_announcements:
        count = len(all_new_announcements)
        print(f"\nì´ {count}ê°œì˜ ì‹ ê·œ ê³µê³ ë¥¼ ë°œê²¬í•˜ì—¬ ìš”ì•½ ì´ë©”ì¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.")
        subject = f"[ì…ì°° ê³µê³ ] {count}ê°œì˜ ì‹ ê·œ ê³µê³ ê°€ ìˆìŠµë‹ˆë‹¤."
        body = generate_summary_email_body(all_new_announcements)
        send_email(subject, body, email_to_receive)
    else:
        print("\nâ„¹ï¸ ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìƒˆë¡œìš´ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()

