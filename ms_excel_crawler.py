import requests
from requests_html import HTMLSession # ë™ì  ì»¨í…ì¸  ë Œë”ë§ì„ ìœ„í•´ requests_html ì‚¬ìš©
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import os
import time
from datetime import datetime, timezone, timedelta
import json
import msal
import re
from dateutil.parser import parse as date_parse # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”ë¥¼ ìœ„í•´ dateutil ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

# --- 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ---
PROCESSED_LINKS_FILE = 'processed_links.txt'

# --- 2. Microsoft Graph API ì—°ë™ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) ---
def get_ms_graph_access_token():
    """Azure ADì—ì„œ MS Graph API ì ‘ê·¼ í† í°ì„ ë°œê¸‰ë°›ìŠµë‹ˆë‹¤."""
    tenant_id = os.environ.get('MS_TENANT_ID')
    client_id = os.environ.get('MS_CLIENT_ID')
    client_secret = os.environ.get('MS_CLIENT_SECRET')

    if not all([tenant_id, client_id, client_secret]):
        print("âŒ MS_TENANT_ID, MS_CLIENT_ID, MS_CLIENT_SECRET Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    authority = f"https://login.microsoftonline.com/{tenant_id}"
    app = msal.ConfidentialClientApplication(
        client_id, authority=authority, client_credential=client_secret
    )
    result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])

    if "access_token" in result:
        print("âœ… MS Graph API í† í° ë°œê¸‰ ì„±ê³µ.")
        return result['access_token']
    else:
        print("âŒ MS Graph API í† í° ë°œê¸‰ ì‹¤íŒ¨:", result.get("error_description"))
        return None

def get_excel_data(access_token, sheet_name):
    """MS Graph APIë¥¼ í†µí•´ Excel ì‹œíŠ¸ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    user_principal_name = os.environ.get('MS_USER_PRINCIPAL_NAME')
    excel_file_path = os.environ.get('MS_EXCEL_FILE_PATH')

    if not all([user_principal_name, excel_file_path]):
        print("âŒ MS_USER_PRINCIPAL_NAME ë˜ëŠ” MS_EXCEL_FILE_PATH Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/drive/root:/{excel_file_path}:/workbook/tables('{sheet_name}')/rows"
    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
    
    try:
        response = requests.get(graph_url, headers=headers)
        response.raise_for_status()
        rows_data = response.json().get('value', [])
        
        header_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/drive/root:/{excel_file_path}:/workbook/tables('{sheet_name}')/headerRowRange"
        header_response = requests.get(header_url, headers=headers)
        header_response.raise_for_status()
        header = header_response.json()['values'][0]

        # ë¹ˆ í–‰('')ì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        records = [dict(zip(header, [val if val != '' else None for val in row['values'][0]])) for row in rows_data]
        print(f"âœ… Excel '{sheet_name}' ì‹œíŠ¸ì—ì„œ {len(records)}ê°œì˜ í–‰ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return records

    except requests.exceptions.HTTPError as e:
        print(f"âŒ Excel '{sheet_name}' ì‹œíŠ¸ ë¡œë“œ ì‹¤íŒ¨ (HTTP {e.response.status_code}): {e.response.text}")
        print("     (Excel íŒŒì¼ ê²½ë¡œ, ì‹œíŠ¸/í‘œ ì´ë¦„, API ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.)")
        return []
    except Exception as e:
        print(f"âŒ Excel '{sheet_name}' ì‹œíŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def save_announcements_to_excel(access_token, announcements):
    """ìƒˆë¡œìš´ ê³µê³ ë¥¼ Excel í…Œì´ë¸”ì˜ ë§¨ ìœ„ì— ì‚½ì…í•©ë‹ˆë‹¤."""
    if not announcements: return
    user_principal_name, excel_file_path = os.environ.get('MS_USER_PRINCIPAL_NAME'), os.environ.get('MS_EXCEL_FILE_PATH')
    sheet_name = "Collected_Announcements"
    graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/drive/root:/{excel_file_path}:/workbook/tables('{sheet_name}')/rows/add"
    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
    kst = timezone(timedelta(hours=9))
    collected_time_kst = datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S')
    rows_to_add = [[collected_time_kst, ann['company'], ann['title'], ann.get('date', 'N/A'), ann['href']] for ann in announcements]
    
    payload = {"values": rows_to_add, "index": 0}
    
    try:
        response = requests.post(graph_url, headers=headers, json=payload)
        response.raise_for_status()
        print("âœ… Excelì— ì‹ ê·œ ê³µê³  ì €ì¥ ì™„ë£Œ.")
    except requests.exceptions.HTTPError as e:
        print(f"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (HTTP {e.response.status_code}): {e.response.text}")
    except Exception as e:
        print(f"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- 3. í¬ë¡¤ëŸ¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ---
def send_email(subject, body, receiver_emails):
    """ì§€ì •ëœ ìˆ˜ì‹ ì ëª©ë¡ì—ê²Œ ì´ë©”ì¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."""
    if not receiver_emails:
        print("ğŸŸ¡ ê²½ê³ : ìˆ˜ì‹ ì ì´ë©”ì¼ ì£¼ì†Œê°€ ì—†ì–´ ì´ë©”ì¼ì„ ë°œì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
        
    print(f"\n--- ì´ë©”ì¼ ë°œì†¡ ì‹œë„ ({', '.join(receiver_emails)}) ---")
    try:
        smtp_user, smtp_password = os.environ.get('GMAIL_USER'), os.environ.get('GMAIL_PASSWORD')
        if not all([smtp_user, smtp_password]):
            print("âŒ GMAIL_USER ë˜ëŠ” GMAIL_PASSWORD Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
    except Exception as e:
        print(f"âŒ GitHub Secrets ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
        
    msg = MIMEText(body, 'html', 'utf-8')
    msg['Subject'] = Header(subject, 'utf-8')
    msg['From'] = smtp_user
    msg['To'] = ", ".join(receiver_emails)
    
    try:
        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls()
            server.login(smtp_user, smtp_password)
            server.sendmail(msg['From'], receiver_emails, msg.as_string())
        print(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {subject}")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def load_processed_links():
    if not os.path.exists(PROCESSED_LINKS_FILE): return set()
    with open(PROCESSED_LINKS_FILE, 'r', encoding='utf-8') as f: return set(line.strip() for line in f)

def save_processed_link(link):
    with open(PROCESSED_LINKS_FILE, 'a', encoding='utf-8') as f: f.write(link + '\n')

def generate_summary_email_body(announcements):
    kst = timezone(timedelta(hours=9))
    html = """<head><style>body{font-family:sans-serif}.container{border:1px solid #ddd;padding:20px;margin:20px;border-radius:8px}h2{color:#005aab}table{width:100%;border-collapse:collapse}th,td{border:1px solid #ddd;padding:12px;text-align:left}th{background-color:#f2f2f2}a{color:#005aab;text-decoration:none}a:hover{text-decoration:underline}.footer{margin-top:20px;font-size:12px;color:#888}</style></head><body><div class="container"><h2>ğŸ“¢ ì‹ ê·œ ê³µê³  ìš”ì•½</h2><p><strong>""" + datetime.now(kst).strftime('%Yë…„ %mì›” %dì¼') + """</strong>ì— ë°œê²¬ëœ ì‹ ê·œ ê³µê³  ëª©ë¡ì…ë‹ˆë‹¤.</p><table><thead><tr><th>íšŒì‚¬ëª…</th><th>ê³µê³ ì¼</th><th>ê³µê³  ì œëª©</th></tr></thead><tbody>"""
    for ann in announcements:
        html += f"""<tr><td>{ann['company']}</td><td>{ann.get('date', 'N/A')}</td><td><a href="{ann['href']}">{ann['title']}</a></td></tr>"""
    html += """</tbody></table><p class="footer">ë³¸ ë©”ì¼ì€ ìë™í™”ëœ ìŠ¤í¬ë¦½íŠ¸ì— ì˜í•´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.</p></div></body>"""
    return html

# --- [ì¶”ê°€ëœ í•¨ìˆ˜] ---
def generate_no_new_announcements_email_body():
    """ì‹ ê·œ ê³µê³ ê°€ ì—†ì„ ë•Œ ë°œì†¡í•  ì´ë©”ì¼ ë³¸ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    kst = timezone(timedelta(hours=9))
    html = """<head><style>body{font-family:sans-serif}.container{border:1px solid #ddd;padding:20px;margin:20px;border-radius:8px}h2{color:#005aab}.footer{margin-top:20px;font-size:12px;color:#888}</style></head><body><div class="container"><h2>ğŸ“ ê¸ˆì¼ ì‹ ê·œ ì…ì°° ê³µê³  ì—†ìŒ</h2><p><strong>""" + datetime.now(kst).strftime('%Yë…„ %mì›” %dì¼') + """</strong> ê¸°ì¤€, ëª¨ë‹ˆí„°ë§ ì¤‘ì¸ ì‚¬ì´íŠ¸ì—ì„œ ìƒˆë¡œìš´ ì…ì°° ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.</p><p class="footer">ë³¸ ë©”ì¼ì€ ìë™í™”ëœ ìŠ¤í¬ë¦½íŠ¸ì— ì˜í•´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.</p></div></body>"""
    return html

def standardize_date(date_str):
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not date_str or not isinstance(date_str, str):
        return "N/A"
    try:
        # ì •ê·œì‹ìœ¼ë¡œ 'YYYY.MM.DD' ë˜ëŠ” 'YYYY-MM-DD' ë“±ì˜ ê¸°ë³¸ í˜•ì‹ë§Œ ì¶”ì¶œ
        match = re.search(r'\d{4}[-.]\d{1,2}[-.]\d{1,2}', date_str)
        if match:
            return date_parse(match.group()).strftime('%Y-%m-%d')
        return date_str # ë§¤ì¹­ë˜ëŠ” í˜•ì‹ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    except Exception:
        return date_str # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

# --- 4. í¬ë¡¤ë§ ì „ëµë³„ í•¸ë“¤ëŸ¬ ---
def handle_css_crawl(target, session):
    """CSS ì„ íƒì ê¸°ë°˜ì˜ ì¼ë°˜ì ì¸ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    url = target.get('url')
    base_url = target.get('base_url', '')
    item_selector = target.get('item_selector')
    title_link_selector = target.get('title_link_selector')
    date_selector = target.get('date_selector')
    js_render = (target.get('js_render') or '').upper() == 'Y'

    company = target.get('company', 'N/A')

    if not all([url, item_selector, title_link_selector]):
        print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì˜ url, item_selector ë˜ëŠ” title_link_selectorê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    try:
        headers = {
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/91.0.4472.124 Safari/537.36'
            )
        }
        
        response = session.get(url, headers=headers, timeout=20)
        response.raise_for_status()

        # --- ì¸ì½”ë”© ë³´ì • ---
        if 'heungkuklife' in url:
            response.encoding = 'EUC-KR'
            print(f"â„¹ï¸ '{company}' ì‚¬ì´íŠ¸ì˜ ì¸ì½”ë”©ì„ EUC-KRë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
        elif 'pikk.co.kr' in url:
            response.encoding = 'utf-8'
            print(f"â„¹ï¸ '{company}' ì‚¬ì´íŠ¸ì˜ ì¸ì½”ë”©ì„ UTF-8ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")

        if js_render:
            print(f"â„¹ï¸ '{company}' ì‚¬ì´íŠ¸ëŠ” JavaScript ë Œë”ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            response.html.render(sleep=3, timeout=20)

        if js_render and hasattr(response, "html") and getattr(response.html, "html", None):
            html_source = response.html.html
        else:
            html_source = response.text

        soup = BeautifulSoup(html_source, 'html.parser')
        items = soup.select(item_selector)

        if not items:
            print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì—ì„œ '{item_selector}' ì„ íƒìì— í•´ë‹¹í•˜ëŠ” í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        announcements = []
        for item in items:
            # 1ì°¨ ì‹œë„: ì •ì˜ëœ title_link_selectorë¡œ ì°¾ê¸°
            title_element = None
            if title_link_selector:
                title_element = item.select_one(title_link_selector)

            # 2ì°¨ fallback: item ìì²´ê°€ <a href="..."> ì¸ ê²½ìš°
            if not title_element:
                if item.name == 'a' and item.get('href'):
                    title_element = item
                else:
                    # 3ì°¨ fallback: item ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ <a href=...> ì‚¬ìš©
                    link_tag = item.find('a', href=True)
                    if link_tag:
                        title_element = link_tag

            if not title_element:
                continue

            href = (title_element.get('href') or '').strip()

            # --- ì œëª© ì¶”ì¶œ ---
            if 'pikk.co.kr' in url:
                title_tag = item.find('h3')
                if title_tag:
                    title = title_tag.get_text(strip=True)
                else:
                    title = title_element.get_text(strip=True)
            else:
                title = title_element.get_text(strip=True)

            # --- [ìˆ˜ì •ëœ ë¶€ë¶„] ë§í¬ ì¶”ì¶œ ë¡œì§ ì™„ì„±í˜• (data-key, href=javascript, onclick ëª¨ë‘ ì§€ì›) ---
            # hrefê°€ ì—†ê±°ë‚˜, javascript, ë˜ëŠ” # ë§í¬ì¸ ê²½ìš° ëŒ€ì²´ ì†ì„± í™•ì¸
            if not href or 'javascript' in href.lower() or href == '#':
                link_format = target.get('link_format')
                
                # 1. data-key ì†ì„± í™•ì¸ (ì‹ í•œë¼ì´í”„ ë“±)
                data_key = title_element.attrs.get('data-key')
                
                if data_key and link_format:
                    href = link_format.replace('{id}', str(data_key).strip())
                
                # 2. ìë°”ìŠ¤í¬ë¦½íŠ¸(onclick ë˜ëŠ” href)ì—ì„œ ID ì¶”ì¶œ (ì‚¼ì–‘ê·¸ë£¹, ë¯¸ë˜ì—ì…‹ ë“±)
                else:
                    # onclick ê°’ì„ ë¨¼ì € ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ href ê°’ì´ 'javascript:'ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                    js_code = (title_element.get('onclick') or '').strip()
                    if not js_code and href.lower().startswith('javascript:'):
                        js_code = href
                    
                    # ì •ê·œì‹ìœ¼ë¡œ ê´„í˜¸ ì•ˆì˜ ìˆ«ìë‚˜ ë¬¸ìì—´ ì¶”ì¶œ (ì˜ˆ: goView(11453) -> 11453)
                    if js_code:
                        match = re.search(r"[(']([^()']+)[')]", js_code)
                        if match:
                            link_part = match.group(1)
                            if link_format:
                                href = link_format.replace('{id}', link_part)

            # ë‚ ì§œ íŒŒì‹±
            post_date = "N/A"
            if date_selector:
                date_element = item.select_one(date_selector)
                if date_element:
                    post_date = standardize_date(date_element.get_text(strip=True))

            # ìƒëŒ€ê²½ë¡œ ë§í¬ë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
            if href and not href.startswith('http') and not href.startswith('javascript'):
                href = (base_url or url).rstrip('/') + '/' + href.lstrip('/')

            if href and title:
                announcements.append({
                    "title": title,
                    "href": href,
                    "date": post_date
                })

        return announcements

    except requests.exceptions.Timeout:
        print(f"âŒ '{company}' ì‚¬ì´íŠ¸ ì ‘ì† ì‹œê°„ ì´ˆê³¼.")
        return []
    except requests.RequestException as e:
        print(f"âŒ '{company}' ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        print(f"âŒ '{company}' ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        return []

def handle_api_crawl(target, session):
    """JSON API ê¸°ë°˜ì˜ í¬ë¡¤ë§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    api_url = target.get('api_url')
    method = (target.get('api_method') or 'GET').upper()
    
    def get_path(path_str):
        return path_str.split('.') if path_str else []

    item_path = get_path(target.get('json_item_path'))
    title_path = get_path(target.get('json_title_path'))
    link_id_path = get_path(target.get('json_link_id_path'))
    date_path = get_path(target.get('json_date_path'))
    link_format = target.get('link_format')

    if not all([api_url, item_path, title_path, link_id_path, link_format]):
        print(f"ğŸŸ¡ ê²½ê³ : '{target.get('company')}'ì˜ API ì„¤ì •ì´ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    try:
        payload_str = target.get('api_payload')
        payload = json.loads(payload_str) if payload_str else None

        if method == 'POST':
            response = session.post(api_url, json=payload, data=None if payload else target.get('api_form_data'))
        else:
            response = session.get(api_url, params=payload)
        
        response.raise_for_status()
        data = response.json()

        items = data
        for key in item_path:
            items = items.get(key, [])
            if not isinstance(items, list):
                print(f"ğŸŸ¡ ê²½ê³ : '{target.get('company')}'ì˜ json_item_path '{'.'.join(item_path)}'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return []
        
        announcements = []
        for item in items:
            title = item
            for key in title_path: title = title.get(key)
            
            link_id = item
            for key in link_id_path: link_id = link_id.get(key)
            
            post_date = "N/A"
            if date_path:
                date_val = item
                for key in date_path: date_val = date_val.get(key)
                if isinstance(date_val, int) and date_val > 10000000000:
                    post_date = datetime.fromtimestamp(date_val / 1000).strftime('%Y-%m-%d')
                else:
                    post_date = standardize_date(str(date_val))

            if title and link_id:
                href = link_format.replace('{id}', str(link_id))
                announcements.append({"title": str(title), "href": href, "date": post_date})
        
        return announcements

    except requests.RequestException as e:
        print(f"âŒ '{target.get('company')}' API ì ‘ì† ì‹¤íŒ¨: {e}")
        return []
    except json.JSONDecodeError:
        print(f"âŒ '{target.get('company')}' API ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"âŒ '{target.get('company')}' API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- 5. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def crawl_site(target, processed_links, session):
    """í¬ë¡¤ë§ ëŒ€ìƒì„ ë¶„ê¸°í•˜ì—¬ ì‹¤í–‰í•˜ê³  ì‹ ê·œ ê³µê³ ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    company = target.get('company', 'N/A')
    crawl_type = (target.get('crawl_type') or 'CSS').upper()

    print(f"\n--- '{company}' ({crawl_type}) ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ---")
    
    new_announcements = []
    if crawl_type == 'CSS':
        results = handle_css_crawl(target, session)
    elif crawl_type == 'API':
        results = handle_api_crawl(target, session)
    else:
        print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì˜ crawl_type '{crawl_type}'ì€ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤.")
        results = []

    if results:
        for ann in results:
            ann['company'] = company
            if ann['href'] and ann['href'] not in processed_links:
                print(f"ğŸš€ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬: [{company}] {ann['title']} (ê³µê³ ì¼: {ann['date']})")
                new_announcements.append(ann)
                save_processed_link(ann['href'])
                processed_links.add(ann['href'])
    
    if not new_announcements:
        print(f"â„¹ï¸ '{company}'ì—ì„œ ìƒˆë¡œìš´ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
    return new_announcements

def main():
    print("="*60 + f"\nì…ì°° ê³µê³  í¬ë¡¤ëŸ¬ (v4.2 - ê³µê³  ì—†ì„ ì‹œì—ë„ ë©”ì¼ ë°œì†¡)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n" + "="*60)
    
    access_token = get_ms_graph_access_token()
    if not access_token: return

    settings_data = get_excel_data(access_token, "Settings")
    settings = {item['Setting']: item['Value'] for item in settings_data if item.get('Setting') and item.get('Value')}
    
    # ì›Œí¬í”Œë¡œìš° íƒ€ì…ì— ë”°ë¼ ìˆ˜ì‹ ì ì´ë©”ì¼ ëª©ë¡ ê²°ì •
    workflow_type = os.environ.get('WORKFLOW_TYPE', 'DEFAULT')
    receiver_emails = []
    
    developer_email = settings.get('Developer Email')
    receiver_email = settings.get('Receiver Email')

    if workflow_type == 'TEST':
        if developer_email:
            receiver_emails.append(developer_email)
        print("â„¹ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰. ê°œë°œìì—ê²Œë§Œ ì´ë©”ì¼ì´ ë°œì†¡ë©ë‹ˆë‹¤.")
    else: # DEFAULT (ì¼ë°˜ ìŠ¤ì¼€ì¤„ ì‹¤í–‰)
        if receiver_email:
            receiver_emails.append(receiver_email)
        if developer_email:
            receiver_emails.append(developer_email)
        print("â„¹ï¸ ì¼ë°˜ ëª¨ë“œë¡œ ì‹¤í–‰. ëª¨ë“  ìˆ˜ì‹ ìì—ê²Œ ì´ë©”ì¼ì´ ë°œì†¡ë©ë‹ˆë‹¤.")
            
    targets = get_excel_data(access_token, "Crawl_Targets")
    
    if not targets or not receiver_emails:
        print("âŒ í¬ë¡¤ë§ì— í•„ìš”í•œ ì„¤ì • ì •ë³´(ëŒ€ìƒ ë˜ëŠ” ìˆ˜ì‹  ì´ë©”ì¼)ê°€ ë¶€ì¡±í•˜ì—¬ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
        
    processed_links = load_processed_links()
    all_new_announcements = []
    
    session = HTMLSession()

    for target in targets:
        if not target.get('company'): 
            continue
        try:
            new_finds = crawl_site(target, processed_links, session)
            if new_finds:
                all_new_announcements.extend(new_finds)
        except Exception as e:
            print(f"ğŸš¨ '{target.get('company')}' í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        time.sleep(1)

    print("\n" + "="*25 + " ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ " + "="*25)

    # --- [ìˆ˜ì •ëœ ë¶€ë¶„] ---
    if all_new_announcements:
        all_new_announcements.sort(key=lambda x: (x.get('date', '0000-00-00'), x.get('company')), reverse=True)
        
        save_announcements_to_excel(access_token, all_new_announcements)
        count = len(all_new_announcements)
        subject = f"[ì‹ ê·œ ê³µê³  ì•Œë¦¼] {count}ê°œì˜ ìƒˆë¡œìš´ ê³µê³ ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        body = generate_summary_email_body(all_new_announcements)
        send_email(subject, body, receiver_emails)
    else:
        # ì‹ ê·œ ê³µê³ ê°€ ì—†ì„ ë•Œë„ ì´ë©”ì¼ì„ ë°œì†¡í•˜ë„ë¡ ë³€ê²½
        print("\nâ„¹ï¸ ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìƒˆë¡œìš´ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ ì´ë©”ì¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.")
        kst = timezone(timedelta(hours=9))
        today_str = datetime.now(kst).strftime('%Y-%m-%d')
        subject = f"[ì…ì°° ê³µê³  ì•Œë¦¼] {today_str} ì‹ ê·œ ê³µê³  ì—†ìŒ"
        body = generate_no_new_announcements_email_body() # ìƒˆë¡œ ì¶”ê°€í•œ í•¨ìˆ˜ í˜¸ì¶œ
        send_email(subject, body, receiver_emails)
        
    print("\n" + "="*30 + " ì‘ì—… ì¢…ë£Œ " + "="*30)

if __name__ == '__main__':
    main()
