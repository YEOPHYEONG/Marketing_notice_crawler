import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import os
import time
from datetime import datetime, timezone, timedelta
import json
import msal  # Microsoft ì¸ì¦ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ---
PROCESSED_LINKS_FILE = 'processed_links.txt'

# --- 2. Microsoft Graph API ì—°ë™ í•¨ìˆ˜ë“¤ ---
def get_ms_graph_access_token():
    """Azure ADì—ì„œ MS Graph API ì ‘ê·¼ í† í°ì„ ë°œê¸‰ë°›ìŠµë‹ˆë‹¤."""
    tenant_id = os.environ.get('MS_TENANT_ID')
    client_id = os.environ.get('MS_CLIENT_ID')
    client_secret = os.environ.get('MS_CLIENT_SECRET')

    if not all([tenant_id, client_id, client_secret]):
        print("âŒ MS_TENANT_ID, MS_CLIENT_ID, MS_CLIENT_SECRET Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    authority = f"https://login.microsoftonline.com/{tenant_id}"
    app = msal.ConfidentialClientApplication(
        client_id, authority=authority, client_credential=client_secret
    )
    result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])

    if "access_token" in result:
        print("âœ… MS Graph API í† í° ë°œê¸‰ ì„±ê³µ.")
        return result['access_token']
    else:
        print("âŒ MS Graph API í† í° ë°œê¸‰ ì‹¤íŒ¨:", result.get("error_description"))
        return None

def get_excel_data(access_token, sheet_name):
    """MS Graph APIë¥¼ í†µí•´ Excel ì‹œíŠ¸ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    user_principal_name = os.environ.get('MS_USER_PRINCIPAL_NAME')
    excel_file_path = os.environ.get('MS_EXCEL_FILE_PATH')

    if not all([user_principal_name, excel_file_path]):
        print("âŒ MS_USER_PRINCIPAL_NAME ë˜ëŠ” MS_EXCEL_FILE_PATH Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/drive/root:/{excel_file_path}:/workbook/tables('{sheet_name}')/rows"
    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
    
    try:
        response = requests.get(graph_url, headers=headers)
        response.raise_for_status()
        rows_data = response.json().get('value', [])
        
        header_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/drive/root:/{excel_file_path}:/workbook/tables('{sheet_name}')/headerRowRange"
        header_response = requests.get(header_url, headers=headers)
        header_response.raise_for_status()
        header = header_response.json()['values'][0]

        records = [dict(zip(header, row['values'][0])) for row in rows_data]
        print(f"âœ… Excel '{sheet_name}' ì‹œíŠ¸ì—ì„œ {len(records)}ê°œì˜ í–‰ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return records

    except requests.exceptions.HTTPError as e:
        print(f"âŒ Excel '{sheet_name}' ì‹œíŠ¸ ë¡œë“œ ì‹¤íŒ¨ (HTTP {e.response.status_code}): {e.response.text}")
        print("   (Excel íŒŒì¼ ê²½ë¡œ, ì‹œíŠ¸/í‘œ ì´ë¦„, API ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.)")
        return []
    except Exception as e:
        print(f"âŒ Excel '{sheet_name}' ì‹œíŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def save_announcements_to_excel(access_token, announcements):
    """[ìˆ˜ì •] ìƒˆë¡œìš´ ê³µê³ ë¥¼ Excel í…Œì´ë¸”ì˜ ë§¨ ìœ„ì— ì‚½ì…í•©ë‹ˆë‹¤."""
    if not announcements: return
    user_principal_name, excel_file_path = os.environ.get('MS_USER_PRINCIPAL_NAME'), os.environ.get('MS_EXCEL_FILE_PATH')
    sheet_name = "Collected_Announcements"
    graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/drive/root:/{excel_file_path}:/workbook/tables('{sheet_name}')/rows/add"
    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
    kst = timezone(timedelta(hours=9))
    collected_time_kst = datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S')
    rows_to_add = [[collected_time_kst, ann['company'], ann['title'], ann.get('date', 'N/A'), ann['href']] for ann in announcements]
    
    # [ìˆ˜ì •] index: 0ì„ ì¶”ê°€í•˜ì—¬ í…Œì´ë¸”ì˜ ë§¨ ìœ„ì— í–‰ì„ ì‚½ì…í•©ë‹ˆë‹¤.
    payload = {"values": rows_to_add, "index": 0}
    
    try:
        response = requests.post(graph_url, headers=headers, json=payload)
        response.raise_for_status()
        print("âœ… Excelì— ì‹ ê·œ ê³µê³  ì €ì¥ ì™„ë£Œ.")
    except requests.exceptions.HTTPError as e:
        print(f"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (HTTP {e.response.status_code}): {e.response.text}")
    except Exception as e:
        print(f"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- 3. í¬ë¡¤ëŸ¬ í•µì‹¬ í•¨ìˆ˜ë“¤ ---
def send_email(subject, body, receiver_email):
    print("\n--- ì´ë©”ì¼ ë°œì†¡ ì‹œë„ ---")
    try:
        smtp_user, smtp_password = os.environ.get('GMAIL_USER'), os.environ.get('GMAIL_PASSWORD')
        if not all([smtp_user, smtp_password]): print("âŒ GMAIL_USER ë˜ëŠ” GMAIL_PASSWORD Secretì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."); return
    except Exception as e: print(f"âŒ GitHub Secrets ë¡œë“œ ì‹¤íŒ¨: {e}"); return
    msg = MIMEText(body, 'html', 'utf-8')
    msg['Subject'], msg['From'], msg['To'] = Header(subject, 'utf-8'), smtp_user, receiver_email
    try:
        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls(); server.login(smtp_user, smtp_password)
            server.sendmail(msg['From'], [msg['To']], msg.as_string())
        print(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {subject}")
    except Exception as e: print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def load_processed_links():
    if not os.path.exists(PROCESSED_LINKS_FILE): return set()
    with open(PROCESSED_LINKS_FILE, 'r', encoding='utf-8') as f: return set(line.strip() for line in f)

def save_processed_link(link):
    with open(PROCESSED_LINKS_FILE, 'a', encoding='utf-8') as f: f.write(link + '\n')

def generate_summary_email_body(announcements):
    kst = timezone(timedelta(hours=9))
    html = """<head><style>body{font-family:sans-serif}.container{border:1px solid #ddd;padding:20px;margin:20px;border-radius:8px}h2{color:#005aab}table{width:100%;border-collapse:collapse}th,td{border:1px solid #ddd;padding:12px;text-align:left}th{background-color:#f2f2f2}a{color:#005aab;text-decoration:none}a:hover{text-decoration:underline}.footer{margin-top:20px;font-size:12px;color:#888}</style></head><body><div class="container"><h2>ğŸ“¢ ì‹ ê·œ ê³µê³  ìš”ì•½</h2><p><strong>""" + datetime.now(kst).strftime('%Yë…„ %mì›” %dì¼') + """</strong>ì— ë°œê²¬ëœ ì‹ ê·œ ê³µê³  ëª©ë¡ì…ë‹ˆë‹¤.</p><table><thead><tr><th>íšŒì‚¬ëª…</th><th>ê³µê³ ì¼</th><th>ê³µê³  ì œëª©</th></tr></thead><tbody>"""
    for ann in announcements:
        html += f"""<tr><td>{ann['company']}</td><td>{ann.get('date', 'N/A')}</td><td><a href="{ann['href']}">{ann['title']}</a></td></tr>"""
    html += """</tbody></table><p class="footer">ë³¸ ë©”ì¼ì€ ìë™í™”ëœ ìŠ¤í¬ë¦½íŠ¸ì— ì˜í•´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.</p></div></body>"""
    return html

def crawl_site(target, processed_links):
    company, url, base_url = target.get('company','N/A'), target.get('url'), target.get('base_url','')
    item_selector, title_link_selector, date_selector = target.get('item_selector'), target.get('title_link_selector'), target.get('date_selector')
    new_announcements = []
    if not all([url, item_selector, title_link_selector]):
        print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì˜ url, item_selector ë˜ëŠ” title_link_selectorê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return new_announcements
    print(f"\n--- '{company}' ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ---")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âŒ '{company}' ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨: {e}"); return new_announcements
    soup = BeautifulSoup(response.text, 'html.parser')
    items = soup.select(item_selector)
    if not items:
        print(f"ğŸŸ¡ ê²½ê³ : '{company}'ì—ì„œ '{item_selector}' ì„ íƒìì— í•´ë‹¹í•˜ëŠ” í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return new_announcements
        
    for item in items:
        title_link_element = item.select_one(title_link_selector)
        if not title_link_element: continue
        title, href = title_link_element.get_text(strip=True), title_link_element.get('href', '')
        post_date = "N/A"
        if date_selector:
            date_element = item.select_one(date_selector)
            if date_element: post_date = date_element.get_text(strip=True)
        if href and not href.startswith('http'):
            href = base_url.rstrip('/') + '/' + href.lstrip('/')
        
        if href and href not in processed_links:
            print(f"ğŸš€ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬: [{company}] {title} (ê³µê³ ì¼: {post_date})")
            new_announcements.append({"company": company, "title": title, "href": href, "date": post_date})
            save_processed_link(href)
            processed_links.add(href)
            
    if not new_announcements: print(f"â„¹ï¸ '{company}'ì—ì„œ ìƒˆë¡œìš´ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    # [ìˆ˜ì •] reverse() ë¡œì§ì„ ì œê±°í•˜ì—¬, ì›¹ì‚¬ì´íŠ¸ì— ë³´ì´ëŠ” ìˆœì„œ (ë³´í†µ ìµœì‹ ìˆœ) ê·¸ëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    return new_announcements

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def main():
    print("="*50 + "\nMS Excel ì—°ë™ ì…ì°° ê³µê³  í¬ë¡¤ëŸ¬ (v3 - ìµœì‹ ê¸€ ìƒë‹¨)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n" + "="*50)
    
    access_token = get_ms_graph_access_token()
    if not access_token: return

    settings_data = get_excel_data(access_token, "Settings")
    settings = {item['Setting']: item['Value'] for item in settings_data if 'Setting' in item and 'Value' in item}
    email_to_receive = settings.get('Receiver Email')

    targets = get_excel_data(access_token, "Crawl_Targets")
    
    if not targets or not email_to_receive:
        print("í¬ë¡¤ë§ì— í•„ìš”í•œ ì„¤ì • ì •ë³´(ëŒ€ìƒ, ìˆ˜ì‹  ì´ë©”ì¼)ê°€ ë¶€ì¡±í•˜ì—¬ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
        
    processed_links = load_processed_links()
    all_new_announcements = []

    for target in targets:
        if not target.get('company'): continue
        new_finds = crawl_site(target, processed_links)
        if new_finds:
            all_new_announcements.extend(new_finds)
        time.sleep(1)

    print("\n--- ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ ---")

    if all_new_announcements:
        save_announcements_to_excel(access_token, all_new_announcements)
        count = len(all_new_announcements)
        subject = f"[ì‹ ê·œ ê³µê³  ì•Œë¦¼] {count}ê°œì˜ ìƒˆë¡œìš´ ê³µê³ ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        body = generate_summary_email_body(all_new_announcements)
        send_email(subject, body, email_to_receive)
    else:
        print("\nâ„¹ï¸ ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìƒˆë¡œìš´ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()

