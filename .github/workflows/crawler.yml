name: Bid Announcement Crawler from Google Sheets

on:
  schedule:
    # UTC 기준 22시는 한국 시간(KST) 오전 7시입니다.
    - cron: '0 22 * * *'
  # GitHub Actions 탭에서 수동으로 실행할 수 있도록 허용
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest

    # [수정] 스크립트가 processed_links.txt 파일을 다시 커밋(쓰기)할 수 있도록 권한을 부여합니다.
    permissions:
      contents: write

    steps:
      # 1. 리포지토리의 코드를 가상 환경으로 가져옵니다.
      - name: Checkout repository
        uses: actions/checkout@v3
        
      # 2. 파이썬 3.10 버전을 설치합니다.
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      # 3. requirements.txt에 명시된 라이브러리들을 설치합니다.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      # 4. 파이썬 스크립트를 실행합니다.
      - name: Run Python script
        env:
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_PASSWORD: ${{ secrets.GMAIL_PASSWORD }}
          GOOGLE_API_CREDENTIALS: ${{ secrets.GOOGLE_API_CREDENTIALS }}
        run: python google_sheets_crawler.py
        
      # 5. 스크립트 실행으로 변경된 processed_links.txt 파일을 GitHub에 다시 저장(커밋)합니다.
      - name: Commit and push if changed
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add processed_links.txt
          # 변경 사항이 있을 때만 커밋하도록 설정
          git diff --staged --quiet || git commit -m "Update processed links"
          git push

